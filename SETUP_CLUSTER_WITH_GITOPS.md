# üöÄ H∆Ø·ªöNG D·∫™N TRI·ªÇN KHAI K3S CLUSTER (MASTER + WORKER)

## B∆Ø·ªöC 2: SET IP Tƒ®NH + DISABLE CLOUD-INIT (MASTER)

Disable cloud-init network:
```bash
sudo nano /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg
```

N·ªôi dung:
```yaml
network: {config: disabled}
```

X√≥a netplan c≈©:
```bash
sudo rm -f /etc/netplan/50-cloud-init.yaml
```

T·∫°o netplan m·ªõi:
```bash
sudo nano /etc/netplan/01-static.yaml
```

N·ªôi dung:
```yaml
network:
  version: 2
  renderer: networkd
  ethernets:
    ens33:
      dhcp4: no
      addresses:
        - 192.168.0.50/24
      gateway4: 192.168.0.1
      nameservers:
        addresses:
          - 8.8.8.8
          - 1.1.1.1
```

Apply:
```bash
sudo netplan apply
```

## B∆Ø·ªöC 1: ƒê·ªîI HOSTNAME (TR√äN NODE MASTER)

Nh·ªõ d√πng `ip a` ƒë·ªÉ check **IP / mask / gateway** v√† thay cho ƒë√∫ng tr∆∞·ªõc khi l√†m b·∫•t c·ª© ƒëi·ªÅu g√¨.
```bash
sudo hostnamectl set-hostname k3s-master
sudo nano /etc/hosts
```

V√≠ d·ª• n·ªôi dung:
```txt
127.0.0.1 localhost
192.168.0.50 k3s-master
```

Reboot:
```bash
sudo reboot
```

Check IP:
```bash
ip a
```

## B∆Ø·ªöC 3: SCAN IP C√ÅC SERVER WORKER (TR√äN MASTER)

C√†i `nmap`:
```bash
sudo apt install nmap -y
```

Auto generate inventory file

‚ö†Ô∏è Nh·ªõ s·ª≠a subnet + port SSH cho ƒë√∫ng m√¥i tr∆∞·ªùng. Sau n√†y th√™m server th√¨ nh·ªõ ch·∫°y l·∫°i c√°i n√†y l√† oke.
```bash
SUBNET=192.168.0.0/24
PORT=8022
USER="thang2k6adu"
START_IP=51
MASTER_IP=$(hostname -I | awk '{print $1}')
BASE_IP=$(echo $SUBNET | cut -d'/' -f1 | awk -F. '{print $1"."$2"."$3}')

mkdir -p ~/k3s-inventory && cd ~/k3s-inventory

echo -e "[master]\n$MASTER_IP ansible_user=$USER ansible_port=$PORT worker_ip=$MASTER_IP\n\n[workers]" > hosts.ini

sudo nmap -p $PORT --open $SUBNET \
| grep "Nmap scan report" \
| grep -oE "([0-9]{1,3}\.){3}[0-9]{1,3}" \
| grep -v "^$MASTER_IP$" \
| awk -v USER="$USER" -v PORT="$PORT" -v BASE="$BASE_IP" -v START="$START_IP" \
'{print $0" ansible_user="USER" ansible_port="PORT" worker_ip="BASE"."START+NR-1}' \
>> hosts.ini

cd ~/
```

Check file inventory:
```bash
cat ~/k3s-inventory/hosts.ini
```

K·∫øt qu·∫£ mong ƒë·ª£i:
```ini
[master]
192.168.0.50 ansible_user=thang2k6adu ansible_port=8022 worker_ip=192.168.0.50

[workers]
192.168.0.108 ansible_user=thang2k6adu ansible_port=8022 worker_ip=192.168.0.51
192.168.0.109 ansible_user=thang2k6adu ansible_port=8022 worker_ip=192.168.0.52
```

## B∆Ø·ªöC 4: C√ÄI K3S CONTROL PLANE (MASTER)

ƒê·∫∑t t√™n node l√† `k3s-master`:
```bash
curl -sfL https://get.k3s.io | sh -s - \
  --write-kubeconfig-mode 644 \
  --node-name k3s-master
```

Check:
```bash
kubectl get nodes
```

## B∆Ø·ªöC 5: M·ªû FIREWALL (UFW)

### Master:
```bash
sudo ufw allow 6443/tcp   # worker k·∫øt n·ªëi v·ªÅ master
sudo ufw allow 8472/udp   # pod giao ti·∫øp
sudo ufw allow 10250/tcp  # l·∫•y log pod
```

### Worker (b·∫±ng Ansible):
```bash
sudo ufw allow 8472/udp
sudo ufw allow 10250/tcp
```

## C√ÄI ANSIBLE TR√äN MASTER
```bash
sudo apt update
sudo apt install ansible -y
```

L∆∞u √Ω ph·∫£i l·∫Øp ssh v√†o master node tr∆∞·ªõc khi ssh

L·∫•y ssh private key ƒë√£ b·ªè v√†o c√°c node (l√∫c setup) r·ªìi b·ªè l√™n master
·ªü ƒë√¢y ch·ªâ c√≥ h∆∞·ªõng d·∫´n window
scp -P 8022 $env:USERPROFILE\.ssh\id_ed25519 thang2k6adu@192.168.0.50
:/home/thang2k6adu/.ssh/id_ed25519

l·∫•y public key b·ªè v√†o
scp -P 8022 $env:USERPROFILE\.ssh\id_ed25519.pub thang2k6adu@192.168.0.50:/home/thang2k6adu/.ssh/id_ed25519.pub

ph√¢n quy·ªÅn
chmod 700 ~/.ssh
chmod 600 ~/.ssh/id_ed25519
```

Test k·∫øt n·ªëi:
```bash
ansible workers -i ~/k3s-inventory/hosts.ini -m ping
```

## SET SUDO KH√îNG PASSWORD (CHO WORKER)

T·∫°o file:
```bash
nano ~/k3s-inventory/setup-sudo.yml
```
```yaml
- hosts: workers
  become: yes
  tasks:
    - name: Allow thang2k6adu sudo without password
      copy:
        dest: /etc/sudoers.d/thang2k6adu
        content: |
          thang2k6adu ALL=(ALL) NOPASSWD:ALL
        owner: root
        group: root
        mode: '0440'
```

Run:
```bash
ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/setup-sudo.yml -K
```

t·∫°o playbook gen card m·∫°ng

nano ~/k3s-inventory/gen_iface.yml

- hosts: master,workers
  gather_facts: yes
  vars:
    inventory_file: "{{ playbook_dir }}/hosts.ini"

  tasks:
    - name: Update inventory with iface
      delegate_to: localhost
      lineinfile:
        path: "{{ inventory_file }}"
        regexp: "^{{ inventory_hostname }}\\s"
        line: "{{ inventory_hostname }} ansible_user={{ ansible_user }} ansible_port={{ ansible_port }} worker_ip={{ hostvars[inventory_hostname].worker_ip }} iface={{ ansible_default_ipv4.interface }}"

check
ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/gen_iface.yml -K

check
cat ~/k3s-inventory/hosts.ini

## SET IP Tƒ®NH CHO WORKER
```bash
nano ~/k3s-inventory/set-static-ip.yml
```
```yaml
- hosts: workers
  become: yes
  vars:
    dns:
      - 8.8.8.8
      - 1.1.1.1

  tasks:
    - name: Disable cloud-init network
      copy:
        dest: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg
        content: |
          network: {config: disabled}

    - name: Remove old netplan config
      file:
        path: /etc/netplan/50-cloud-init.yaml
        state: absent

    - name: Configure static IP
      template:
        src: static.yaml.j2
        dest: /etc/netplan/01-static.yaml
        mode: '0644'

    - name: Apply netplan
      command: netplan apply
      async: 10
      poll: 0
```

```bash
nano ~/k3s-inventory/static.yaml.j2
```

```yaml
network:
  version: 2
  renderer: networkd
  ethernets:
    {{ hostvars[inventory_hostname].iface }}:
      dhcp4: no
      addresses:
        - {{ hostvars[inventory_hostname].worker_ip }}/24
      routes:
        - to: default
          via: {{ ansible_default_ipv4.gateway }}
      nameservers:
        addresses:
{% for d in dns %}
          - {{ d }}
{% endfor %}
```

Run:
```bash
ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/set-static-ip.yml
```

gen l·∫°i host
```bash
SUBNET=192.168.0.0/24
PORT=8022
USER="thang2k6adu"
START_IP=51
MASTER_IP=$(hostname -I | awk '{print $1}')
BASE_IP=$(echo $SUBNET | cut -d'/' -f1 | awk -F. '{print $1"."$2"."$3}')

mkdir -p ~/k3s-inventory && cd ~/k3s-inventory

echo -e "[master]\n$MASTER_IP ansible_user=$USER ansible_port=$PORT worker_ip=$MASTER_IP\n\n[workers]" > hosts.ini

sudo nmap -p $PORT --open $SUBNET \
| grep "Nmap scan report" \
| grep -oE "([0-9]{1,3}\.){3}[0-9]{1,3}" \
| grep -v "^$MASTER_IP$" \
| awk -v USER="$USER" -v PORT="$PORT" -v BASE="$BASE_IP" -v START="$START_IP" \
'{print $0" ansible_user="USER" ansible_port="PORT" worker_ip="BASE"."START+NR-1}' \
>> hosts.ini

cd ~/

ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/gen_iface.yml -K
```

Check file inventory:
```bash
cat ~/k3s-inventory/hosts.ini
```

Check:
```bash
ansible workers -i ~/k3s-inventory/hosts.ini -m shell -a \
"echo '=== HOST:' \$(hostname) && ip a | grep inet && ip route | grep default && ping -c 2 8.8.8.8"
```

## M·ªû FIREWALL CHO WORKER (ANSIBLE)
```bash
nano ~/k3s-inventory/open-ufw-worker.yml
```
```yaml
- hosts: workers
  become: yes
  tasks:
    - name: Allow flannel VXLAN (8472/udp)
      ufw:
        rule: allow
        port: 8472
        proto: udp

    - name: Allow kubelet API (10250/tcp)
      ufw:
        rule: allow
        port: 10250
        proto: tcp

    - name: Enable UFW
      ufw:
        state: enabled
```

Run:
```bash
ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/open-ufw-worker.yml
```

ƒë·ªïi t√™n node tr∆∞·ªõc khi join ƒë·ªÉ tr√°nh tr√πng t√™n

nano ~/k3s-inventory/set-hostname.yml

- hosts: workers
  become: yes
  gather_facts: yes

  tasks:
    - name: Set hostname based on last octet of IP
      hostname:
        name: "k3s-worker-{{ ansible_default_ipv4.address.split('.')[-1] }}"

    - name: Update /etc/hosts
      lineinfile:
        path: /etc/hosts
        regexp: "^{{ ansible_default_ipv4.address }}"
        line: "{{ ansible_default_ipv4.address }} k3s-worker-{{ ansible_default_ipv4.address.split('.')[-1] }}"
        state: present

    - name: Reboot to apply hostname
      reboot:
        reboot_timeout: 300

ch·∫°y
ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/set-hostname.yml -K


## L·∫§Y TOKEN T·ª™ MASTER
```bash
sudo cat /var/lib/rancher/k3s/server/node-token
```

V√≠ d·ª•:
```
K10a3f9c8c7b2a3b7f9::server:xxxxxxxx
```

## C√ÄI K3S AGENT (WORKER)
```bash
nano ~/k3s-inventory/install-k3s-worker.yml
```
```yaml
- hosts: workers
  become: yes
  vars:
    k3s_url: "https://192.168.0.50:6443"
    k3s_token: "K10e6dd53c7c99770339ed79f4771c7ded0fbeee5baadfa6ed8224b56a80d5f43ce::server:78b31cbc69888b6ad8603eeb988b07a9"

  tasks:
    - name: Install k3s agent
      shell: |
        curl -sfL https://get.k3s.io | K3S_URL={{ k3s_url }} K3S_TOKEN={{ k3s_token }} sh -
```

Run:
```bash
ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/install-k3s-worker.yml
```

Uninstall n·∫øu l·ªói:
```bash
nano ~/k3s-inventory/uninstall-k3s-worker.yml
```
```yaml
- hosts: workers
  become: yes

  tasks:
    - name: Stop k3s-agent service
      systemd:
        name: k3s-agent
        state: stopped
        enabled: false
      ignore_errors: yes

    - name: Run k3s-agent uninstall script
      shell: |
        if [ -f /usr/local/bin/k3s-agent-uninstall.sh ]; then
          /usr/local/bin/k3s-agent-uninstall.sh
        fi
      args:
        warn: false
      ignore_errors: yes

    - name: Remove k3s directories
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - /etc/rancher/k3s
        - /var/lib/rancher/k3s
        - /var/lib/kubelet
      ignore_errors: yes
```
```bash
ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/uninstall-k3s-worker.yml
```

## CHECK NODE ƒê√É JOIN
```bash
kubectl get nodes -o wide
```

Output:
```
NAME         STATUS   ROLES           IP
k3s-master   Ready    control-plane   192.168.0.50
worker1      Ready    <none>          192.168.0.505
worker2      Ready    <none>          192.168.0.506
```

## SET ROLE CHO WORKER
```bash
kubectl get nodes --no-headers | awk '{print $1}' | grep -v master | xargs -I {} kubectl label node {} node-role.kubernetes.io/worker=worker
```

Check:
```bash
kubectl get nodes
```

Output:
```
NAME            STATUS   ROLES    AGE
192.168.0.505   Ready    worker   1d
192.168.0.506   Ready    worker   1d
```

# üöÄ C√ÄI HELM + KUBERNETES DASHBOARD

## 1Ô∏è‚É£ C√†i Helm
```bash
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
helm version
```

## 2Ô∏è‚É£ C√†i Kubernetes Dashboard
```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
```

Check:
```bash
kubectl get pods -n kubernetes-dashboard
```

## 3Ô∏è‚É£ T·∫°o ServiceAccount (t√†i kho·∫£n cho service)

T·∫°o file:
```bash
nano ~/k3s-inventory/dashboard-admin.yaml
```

N·ªôi dung:
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kubernetes-dashboard-admin
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kubernetes-dashboard-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: kubernetes-dashboard-admin
  namespace: kubernetes-dashboard
```

Apply:
```bash
kubectl apply -f ~/k3s-inventory/dashboard-admin.yaml
```

Check service:
```bash
kubectl get svc -n kubernetes-dashboard
```

## 4Ô∏è‚É£ M·ªü proxy ƒë·ªÉ truy c·∫≠p Dashboard
```bash
sudo ufw allow 8001
kubectl proxy --address=0.0.0.0 --accept-hosts='^.*$'
```

N·∫øu kh√¥ng m·ªü proxy t·∫°i port `8001` th√¨ ph·∫£i v√†o `6443` (ch·∫Øc ch·∫Øn kh√¥ng v√†o ƒë∆∞·ª£c).

Truy c·∫≠p Dashboard:
```
http://192.168.0.50:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
```

Gi·∫£i th√≠ch:

> "API Server, h√£y forward request n√†y t·ªõi Service kubernetes-dashboard, port t√™n l√† https (443), n√≥ l√† port"

## 5Ô∏è‚É£ L·∫•y token ƒë·ªÉ login Dashboard
```bash
kubectl -n kubernetes-dashboard create token kubernetes-dashboard-admin
```

## 6Ô∏è‚É£ N·∫øu SSH th√¨ t·∫°m m·ªü port 8001
```bash
sudo ufw allow 8001
sudo ufw reload
```

Sau khi d√πng xong th√¨ ƒë√≥ng l·∫°i:
```bash
sudo ufw delete allow 8001
sudo ufw reload
```

T·∫•t c·∫£ pod ·ªü node n√†o?
```bash
kubectl get pods -A -o wide
```

## TEST DEPLOY NGINX + NODE PORT
```bash
kubectl create namespace test-nginx
```

L·ªánh n√†y t·∫°o deployment tr√™n node b·∫•t k√¨ (schedule t·ª± ch·ªçn t·ªëi ∆∞u):
```bash
kubectl create deployment nginx \
  --image=nginx \
  -n test-nginx
```

Check:
```bash
kubectl get pods -n test-nginx
```

### Expose

N√†y gi·ªëng t·∫°o 1 service port 80, node port b·∫•t k√¨ tr·ªè v·ªÅ nginx. N√≥ s·∫Ω m·ªü port c·ªßa t·∫•t c·∫£ c√°c node.

YAML ph·∫£i type node port, kh√¥ng l√† n√≥ v·ªÅ ClusterIP:
```bash
kubectl expose deployment nginx \
  --type=NodePort \
  --port=80 \
  -n test-nginx
```

Check:
```bash
kubectl get svc -n test-nginx
```

Output:
```
nginx   NodePort   10.43.7.190   <none>        80:30582/TCP   11s
```

V√†o:
```
http://192.168.0.505:30582
```

### Scale th·ª≠
```bash
kubectl scale deployment -n test-nginx nginx --replicas=3
kubectl get pods -n test-nginx -o wide
```

### Rollback
```bash
kubectl delete namespace test-nginx
```

## SETUP INGRESS (KH√îNG C·∫¶N NODEPORT N·ªÆA)

Gh√©t traefik n√™n disable ƒëi:
```bash
sudo nano /etc/rancher/k3s/config.yaml
```

N·ªôi dung:
```yaml
disable:
  - traefik
```
```bash
sudo systemctl restart k3s
```

Check:
```bash
kubectl get pods -n kube-system
```

Config kube:
```bash
mkdir -p ~/.kube
sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
sudo chown $USER:$USER ~/.kube/config
```

fix l·ªói 127.0.0.1
echo 'export KUBECONFIG=/etc/rancher/k3s/k3s.yaml' >> ~/.bashrc
source ~/.bashrc

### Tr∆∞·ªõc khi c√†i nginx, c√†i monitoring
```bash
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

kubectl create namespace monitoring

helm install monitoring prometheus-community/kube-prometheus-stack \
  -n monitoring
```

Check:
```bash
kubectl get pods -n monitoring
```

Output:
```
prometheus-...
grafana-...
alertmanager-...
node-exporter-...
```

### C√†i nginx
```bash
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update
```

C√°i n√†y cho reverse proxy, c√≤n cloud c√≥ LB s·∫µn n√™n l√† kh√°c:
```bash
mkdir -p ~/k3s-inventory/nginx-ingress-config
nano ~/k3s-inventory/nginx-ingress-config/values.yaml
```

N·ªôi dung:
```yaml
controller:
  replicaCount: 2

  ingressClassResource:
    enabled: true
    default: true
    name: nginx

  kind: Deployment

  service:
    enabled: true
    type: NodePort
    externalTrafficPolicy: Local
    ports:
      http: 80
      https: 443
    nodePorts:
      http: 30080
      https: 30443

  resources:
    requests:
      cpu: 200m
      memory: 256Mi

  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 5
    targetCPUUtilizationPercentage: 60

  config:
    use-forwarded-headers: "true"
    proxy-real-ip-cidr: "0.0.0.0/0"
    real-ip-header: "X-Forwarded-For"
    proxy-body-size: "50m"
    proxy-read-timeout: "600"
    proxy-send-timeout: "600"
    worker-shutdown-timeout: "240s"
    enable-underscores-in-headers: "true"

  allowSnippetAnnotations: false

  metrics:
    enabled: true
    service:
      enabled: true
    serviceMonitor:
      enabled: true

  podDisruptionBudget:
    enabled: true
    minAvailable: 1

  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/component
                  operator: In
                  values:
                    - controller
            topologyKey: kubernetes.io/hostname

  terminationGracePeriodSeconds: 300

  lifecycle:
    preStop:
      exec:
        command:
          - /wait-shutdown

defaultBackend:
  enabled: true
```

Install:
```bash
kubectl create namespace ingress-nginx
helm install ingress-nginx ingress-nginx/ingress-nginx \
  -n ingress-nginx \
  -f ~/k3s-inventory/nginx-ingress-config/values.yaml
```

N·∫øu l·ªói:
```bash
helm uninstall ingress-nginx -n ingress-nginx
kubectl delete namespace ingress-nginx
```

Check:
```bash
kubectl get pods -n ingress-nginx -o wide
kubectl get svc -n ingress-nginx
```

### L√†m l·∫°i nh∆∞ c≈©, kh√°c l√† service l√∫c n√†y l√† Cluster IP ch·ª© kh√¥ng d√πng node port
```bash
kubectl create namespace test-nginx

kubectl create deployment nginx \
  --image=nginx \
  -n test-nginx
```

Kh√°c n√® (kh√¥ng ghi type th√¨ l√† ClusterIP), kh√¥ng name th√¨ c√πng t√™n v·ªõi deployment. Kh√¥ng ƒë·ªãnh nghƒ©a target port th√¨ t·ª± l·∫•y trong deployment:
```bash
kubectl expose deployment nginx \
  --port=80 \
  --target-port=80 \
  -n test-nginx
```
```bash
kubectl get svc -n test-nginx
```
```bash
mkdir ~/k8s-manifest
nano ~/k8s-manifest/nginx-ingress.yaml
```

Prefix s·∫Ω match v·ªõi t·∫•t c·∫£:
```
http://nginx.local/
http://nginx.local/abc
http://nginx.local/api
http://nginx.local/test/123
```

ƒê·ªÅu v√†o nginx h·∫øt:
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-ingress
  namespace: test-nginx
spec:
  rules:
  - host: kruzetech.dev
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx
            port:
              number: 80
```
```bash
kubectl apply -f ~/k8s-manifest/nginx-ingress.yaml
```

Check:
```bash
kubectl get ingress -n test-nginx
```

### Map domain v√†o DNS ·ªü host

V√≠ d·ª• Windows, c√≤n Linux kh√° d·ªÖ th√¥i.

Ch·∫°y PowerShell b·∫±ng admin:
```powershell
notepad C:\Windows\System32\drivers\etc\hosts
```

Th√™m v√†o:
```
192.168.0.505 nginx.local
```

(L∆∞u √Ω l√† ch·ªâ node n√†o c√≥ pod m·ªõi ƒë∆∞·ª£c)

Flush DNS (x√≥a cache):
```powershell
ipconfig /flushdns
```

Ping th·ª≠ ph√°t:
```powershell
ping nginx.local
```
```bash
sudo ufw allow 80
sudo ufw allow 443
```

V√†o:
```
http://nginx.local
```

